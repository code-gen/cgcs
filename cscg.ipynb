{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'language_model/')\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codeop\n",
    "import os\n",
    "import pprint\n",
    "from copy import deepcopy\n",
    "\n",
    "import pandas as pd\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as O\n",
    "from namespace import Namespace\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from dataset import StandardDataset\n",
    "from language_model.lm_train import train_language_model\n",
    "\n",
    "pp = pprint.PrettyPrinter(width=180, indent=2, compact=False)\n",
    "print(f'GPU: {T.cuda.is_available()} | CUDA: {T.version.cuda}')\n",
    "\n",
    "def from_home(x):\n",
    "    return os.path.join(os.environ['HOME'], x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR   = from_home('workspace/ml-data/msc-research')\n",
    "\n",
    "# DJANGO_DIR = os.path.join(ROOT_DIR, 'raw-datasets/testing') # simple django\n",
    "DJANGO_DIR = os.path.join(ROOT_DIR, 'raw-datasets/django')\n",
    "CONALA_DIR = os.path.join(ROOT_DIR, 'raw-datasets/conala-corpus')\n",
    "\n",
    "DATASET_DIR = DJANGO_DIR\n",
    "EMB_DIR     = os.path.join(ROOT_DIR, 'embeddings')\n",
    "\n",
    "print(f'Dataset: {os.path.basename(DATASET_DIR)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [len(l.strip().split()) for l in open(DATASET_DIR + '/all.anno').readlines()]\n",
    "c = [len(l.strip().split()) for l in open(DATASET_DIR + '/all.code').readlines()]\n",
    "assert len(a) == len(c)\n",
    "\n",
    "d = pd.DataFrame([{'a': _a, 'c': _c} for (_a, _c) in zip(a, c)])\n",
    "d.describe()\n",
    "\n",
    "a = round(len(list(filter(lambda x: x <= 24, a))) / len(a), 3)\n",
    "c = round(len(list(filter(lambda x: x <= 20, c))) / len(c), 3)\n",
    "a, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Construct config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = Namespace() # main config\n",
    "\n",
    "# sub-config for dataset\n",
    "CFG.dataset_cfg = Namespace()\n",
    "CFG.dataset_cfg.__dict__ = {\n",
    "    'root_dir': DATASET_DIR,\n",
    "    'anno_min_freq': 10,\n",
    "    'code_min_freq': 10,\n",
    "    'anno_seq_maxlen': 24,\n",
    "    'code_seq_maxlen': 20,\n",
    "    'emb_file': os.path.join(EMB_DIR, 'glove.6B.200d-ft-9-1.txt.pickle'),\n",
    "}\n",
    "\n",
    "dataset = StandardDataset(config=CFG.dataset_cfg, shuffle_at_init=True, seed=42)\n",
    "\n",
    "# sub-config for NL intents\n",
    "CFG.anno = Namespace() \n",
    "CFG.anno.__dict__ = {\n",
    "    'lstm_hidden_size': 64,\n",
    "    'lstm_dropout_p': 0.2,\n",
    "    'att_dropout_p': 0.1,\n",
    "    'lang': dataset.anno_lang,\n",
    "    'load_pretrained_emb': True,\n",
    "    'emb_size': 200,\n",
    "}\n",
    "\n",
    "# sub-config for source code\n",
    "CFG.code = Namespace() \n",
    "CFG.code.__dict__ = {\n",
    "    'lstm_hidden_size': 64,\n",
    "    'lstm_dropout_p': 0.2,\n",
    "    'att_dropout_p': 0.1,\n",
    "    'lang': dataset.code_lang,\n",
    "    'load_pretrained_emb': False,\n",
    "    'emb_size': 32,\n",
    "}\n",
    "\n",
    "CFG.__dict__.update({\n",
    "    'exp_name': f'{os.path.basename(DATASET_DIR)}-p{0}-a{1}',\n",
    "    'cuda': True,\n",
    "    'batch_size': 128,\n",
    "    'num_epochs': 50,\n",
    "    'train_split': 0.7,\n",
    "})\n",
    "\n",
    "print(f'Dataset: {os.path.basename(CFG.dataset_cfg.root_dir)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dir = f'./experiments/{CFG.exp_name}'\n",
    "log_dir = os.path.join(exp_dir, 'tb_logs')\n",
    "os.makedirs(exp_dir, exist_ok=False)\n",
    "tb_writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "exp_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toks = dataset.code_lang.to_numeric('return dict', tokenize_mode='anno', pad_mode='post', max_len=10)\n",
    "# ws = dataset.code_lang.to_tokens(T.tensor(toks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = np.random.randint(len(dataset))\n",
    "# a, c = dataset[i]\n",
    "# assert len(a) == CFG.dataset_cfg.anno_seq_maxlen, f'{i}'\n",
    "# assert len(c) == CFG.dataset_cfg.code_seq_maxlen, f'{i}'\n",
    "# pp.pprint(a)\n",
    "# pp.pprint(dataset.anno_lang.to_tokens(a))\n",
    "# print('-'*120)\n",
    "# pp.pprint(c)\n",
    "# pp.pprint(dataset.code_lang.to_tokens(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# 2. Compute LM probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Get train/test/valid splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tp, _vp = 0.1, 0.2\n",
    "splits = dataset.train_test_valid_split(test_p=_tp, valid_p=_vp, seed=42)\n",
    "\n",
    "for kind in splits:\n",
    "    for t in splits[kind]:\n",
    "        vs = splits[kind][t]\n",
    "        vs = T.cat(vs)\n",
    "        vs = vs[vs != 0]\n",
    "        splits[kind][t] = vs\n",
    "        \n",
    "print(f'train {(1-_tp-_vp)*len(dataset):.2f} | test {_tp*len(dataset)} | dev {_vp*len(dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Train language model\n",
    "\n",
    "**Note:** Must do this for both anno and code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG.language_model = Namespace()\n",
    "CFG.language_model.__dict__ = {\n",
    "    'dataset'     : os.path.basename(DATASET_DIR),\n",
    "    'model'       : 'LSTM', # type of recurrent net (RNN_TANH, RNN_RELU, LSTM, GRU, Transformer)\n",
    "    'n_head'      : None,   # number of heads in the enc/dec of the Transformers\n",
    "    'emb_size'    : 32,     # size of the word embeddings\n",
    "    'n_hid'       : 64,     # number of hidden units per layer\n",
    "    'n_layers'    : 1,      # number of layers\n",
    "    'lr'          : 0.25,    # initial learning rate\n",
    "    'clip'        : 0.25,   # gradient clipping\n",
    "    'dropout_p'   : 0.05,    # dropout applied to layers\n",
    "    'tied'        : False,  # whether to tie the word embeddings and softmax weights\n",
    "    'log_interval': 100,\n",
    "    'epochs'      : 500, # upper epoch limit\n",
    "    'batch_size'  : 128,\n",
    "    'seed'        : None # for reproducibility\n",
    "}\n",
    "\n",
    "# CFG.language_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_cfg = CFG.language_model\n",
    "\n",
    "for kind in ['anno', 'code']:\n",
    "    print(f'Training LM for {kind}\\n')\n",
    "\n",
    "    lm_cfg.kind = kind\n",
    "    lm_cfg.bptt = CFG.dataset_cfg.__dict__[f'{kind}_seq_maxlen'] # seq len\n",
    "    lm_cfg.save_path = f'./data/lm/lm-{lm_cfg.kind}-{lm_cfg.dataset}-epochs{lm_cfg.epochs}.pt' # path to save the final model\n",
    "    \n",
    "#     train_language_model(lm_cfg, \n",
    "#                          num_tokens=len(getattr(dataset, f'{kind}_lang')),\n",
    "#                          train_nums=T.stack(splits[kind]['train']),\n",
    "#                          test_nums=T.stack(splits[kind]['test']),\n",
    "#                          valid_nums=T.stack(splits[kind]['valid']))\n",
    "    \n",
    "    train_language_model(lm_cfg, \n",
    "                         num_tokens=len(getattr(dataset, f'{kind}_lang')),\n",
    "                         train_nums=splits[kind]['train'],\n",
    "                         test_nums=splits[kind]['test'],\n",
    "                         valid_nums=splits[kind]['valid'])\n",
    "    \n",
    "    print('*' * 120, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Compute LM probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_cfg = CFG.language_model\n",
    "lm_paths = {k: f'./data/lm/lm-{k}-{lm_cfg.dataset}-epochs{lm_cfg.epochs}.pt' for k in ['anno', 'code']}\n",
    "\n",
    "for f in lm_paths.values():\n",
    "    assert os.path.exists(f), f'Language Model: file <{f}> does not exist!'\n",
    "    \n",
    "_ = dataset.compute_lm_probs(lm_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(len(dataset))\n",
    "a, c, pa, pc = dataset[i]\n",
    "' '.join(dataset.anno_lang.to_tokens(a)[0]), ' '.join(dataset.code_lang.to_tokens(c)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MyLMProb:\n",
    "#     def __init__(self, model_path):        \n",
    "#         self.model = T.load(open(model_path, 'rb'), map_location={'cuda:0': 'cpu'})\n",
    "#         self.model = self.model.cpu()\n",
    "#         self.model.eval()\n",
    "\n",
    "#     def get_prob(self, nums, verbose=False):\n",
    "#         with T.no_grad():\n",
    "#             inp = T.tensor([int(nums[0])]).long().unsqueeze(0)\n",
    "#             hidden = self.model.init_hidden(bsz=1)\n",
    "#             log_probs = []\n",
    "            \n",
    "#             for i in range(1, len(nums)):\n",
    "#                 output, hidden = self.model(inp, hidden)\n",
    "                \n",
    "#                 #word_weights = output.squeeze().data.double().exp()\n",
    "#                 #prob = word_weights[nums[i]] / word_weights.sum()\n",
    "#                 probs = F.softmax(output.squeeze(), dim=-1)\n",
    "#                 prob = probs[nums[i]]\n",
    "                \n",
    "#                 # append current log prob\n",
    "#                 log_probs += [T.log(prob)]\n",
    "#                 inp.data.fill_(int(nums[i]))\n",
    "\n",
    "#             if verbose:\n",
    "#                 for i in range(len(log_probs)):\n",
    "#                     print(f'{nums[i+1]:4d}: P(w|s) = {np.exp(log_probs[i]):8.4f} | logP(w|s) = {log_probs[i]:8.4f}')\n",
    "#                 print(f'=> sum_prob = {sum(log_probs):.4f}')\n",
    "\n",
    "#         return sum(log_probs) / len(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm_probs = {'anno': [], 'code': []}\n",
    "\n",
    "# pad_idx = {\n",
    "#     'anno': dataset.anno_lang.token2index['<pad>'],\n",
    "#     'code': dataset.code_lang.token2index['<pad>']\n",
    "# } \n",
    "\n",
    "# for kind in lm_probs:\n",
    "#     lm = MyLMProb(lm_paths[kind])\n",
    "#     p = pad_idx[kind]\n",
    "\n",
    "#     for vec in tqdm(getattr(dataset, kind), total=len(dataset), desc=f'P({kind})'):\n",
    "#         lm_probs[kind] += [np.exp(lm.get_prob(vec[vec != pad_idx[kind]], verbose=False))]\n",
    "    \n",
    "#     lm_probs[kind] = sum(lm_probs[kind])\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kind = 'anno'\n",
    "# lm = MyLMProb(lm_paths[kind])\n",
    "# s = {}\n",
    "# for t, i in tqdm(getattr(dataset, f'{kind}_lang').token2index.items()):\n",
    "#     if i in [0, 2, 3]:\n",
    "#         continue\n",
    "#     q = T.tensor([2, i, 3])\n",
    "#     s[i] = np.exp(lm.get_prob(q))\n",
    "    \n",
    "# xs, ys = zip(*sorted(s.items(), key=lambda k: -k[1]))\n",
    "\n",
    "# plt.figure(figsize=(14,6))\n",
    "# plt.bar(xs, ys)\n",
    "# plt.xticks(xs, rotation=90)\n",
    "\n",
    "# sum(ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dual CS/CG Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(config: Namespace):\n",
    "    emb = nn.Embedding(len(config.lang), config.emb_size, padding_idx=config.lang.pad_idx)\n",
    "    \n",
    "    if config.load_pretrained_emb:\n",
    "        assert config.lang.emb_matrix is not None\n",
    "        emb.weight = nn.Parameter(T.tensor(config.lang.emb_matrix, dtype=T.float32))\n",
    "        emb.weight.requires_grad = False\n",
    "        \n",
    "    return emb\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, config: Namespace, model_type):\n",
    "        \"\"\"\n",
    "        :param model_type: cs / cg\n",
    "        cs: code -> anno\n",
    "        cg: anno -> code\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        assert model_type in ['cs', 'cg']\n",
    "        self.model_type = model_type\n",
    "        \n",
    "        src_cfg = config.anno if model_type == 'cg' else config.code\n",
    "        tgt_cfg = config.code if model_type == 'cg' else config.anno\n",
    "        \n",
    "        # 1. ENCODER\n",
    "        self.src_embedding = get_embeddings(src_cfg)\n",
    "        self.encoder = nn.LSTM(input_size=src_cfg.emb_size,\n",
    "                               hidden_size=src_cfg.lstm_hidden_size,\n",
    "                               dropout=src_cfg.lstm_dropout_p,\n",
    "                               bidirectional=True,\n",
    "                               batch_first=True)\n",
    "        \n",
    "        self.decoder_cell_init_linear = nn.Linear(in_features=2*src_cfg.lstm_hidden_size,\n",
    "                                                  out_features=tgt_cfg.lstm_hidden_size)\n",
    "        \n",
    "        # 2. ATTENTION\n",
    "        # project source encoding to decoder rnn's h space (W from Luong score general)\n",
    "        self.att_src_W = nn.Linear(in_features=2*src_cfg.lstm_hidden_size,\n",
    "                                   out_features=tgt_cfg.lstm_hidden_size,\n",
    "                                   bias=False)\n",
    "        \n",
    "        # transformation of decoder hidden states and context vectors before reading out target words\n",
    "        # this produces the attentional vector in (W from Luong eq. 5)\n",
    "        self.att_vec_W = nn.Linear(in_features=2*src_cfg.lstm_hidden_size + tgt_cfg.lstm_hidden_size,\n",
    "                                   out_features=tgt_cfg.lstm_hidden_size,\n",
    "                                   bias=False)\n",
    "        \n",
    "        # 3. DECODER\n",
    "        self.tgt_embedding = get_embeddings(tgt_cfg)\n",
    "        self.decoder = nn.LSTMCell(input_size=tgt_cfg.emb_size + tgt_cfg.lstm_hidden_size,\n",
    "                                   hidden_size=tgt_cfg.lstm_hidden_size)\n",
    "       \n",
    "        # prob layer over target language\n",
    "        self.readout = nn.Linear(in_features=tgt_cfg.lstm_hidden_size,\n",
    "                                 out_features=len(tgt_cfg.lang),\n",
    "                                 bias=False)\n",
    "        \n",
    "        self.dropout = nn.Dropout(tgt_cfg.att_dropout_p)\n",
    "        \n",
    "        # 4. COPY MECHANISM\n",
    "        self.copy_gate = ... # TODO\n",
    "        \n",
    "        # save configs\n",
    "        self.src_cfg = src_cfg\n",
    "        self.tgt_cfg = tgt_cfg\n",
    "        \n",
    "        device = T.device('cuda' if CFG.cuda else 'cpu')\n",
    "        self.to(device)\n",
    "        print(f'[{model_type}] using [{device}]')\n",
    "        \n",
    "        \n",
    "    def forward(self, src, tgt):\n",
    "        \"\"\"\n",
    "        src: bs, max_src_len\n",
    "        tgt: bs, max_tgt_len\n",
    "        \"\"\"\n",
    "        enc_out, (h0_dec, c0_dec) = self.encode(src)\n",
    "        scores, att_mats = self.decode(enc_out, h0_dec, c0_dec, tgt)\n",
    "        \n",
    "        return scores, att_mats\n",
    "    \n",
    "    \n",
    "    def encode(self, src):\n",
    "        \"\"\"\n",
    "        src : bs x max_src_len (emb look-up indices)\n",
    "        out : bs x max_src_len x 2*hid_size\n",
    "        h/c0: bs x tgt_hid_size\n",
    "        \"\"\"\n",
    "        emb = self.src_embedding(src)\n",
    "        out, (hn, cn) = self.encoder(emb) # hidden is zero by default\n",
    "        \n",
    "        # construct initial state for the decoder\n",
    "        c0_dec = self.decoder_cell_init_linear(T.cat([cn[0], cn[1]], dim=1))\n",
    "        h0_dec = c0_dec.tanh()\n",
    "        \n",
    "        return out, (h0_dec, c0_dec)\n",
    "    \n",
    "    \n",
    "    def decode(self, src_enc, h0_dec, c0_dec, tgt):\n",
    "        \"\"\"\n",
    "        src_enc: bs, max_src_len, 2*hid_size (== encoder output)\n",
    "        h/c0   : bs, tgt_hid_size\n",
    "        tgt    : bs, max_tgt_len (emb look-up indices)\n",
    "        \"\"\"\n",
    "        batch_size, tgt_len = tgt.shape\n",
    "        scores, att_mats = [], []\n",
    "        \n",
    "        hidden = (h0_dec, c0_dec)\n",
    "        \n",
    "        emb = self.tgt_embedding(tgt) # bs, max_tgt_len, tgt_emb_size\n",
    "        \n",
    "        att_vec = T.zeros(batch_size, self.tgt_cfg.lstm_hidden_size, requires_grad=False)\n",
    "        if CFG.cuda:\n",
    "            att_vec = att_vec.cuda()\n",
    "        \n",
    "        # Luong W*hs: same for each timestep of the decoder\n",
    "        src_enc_att = self.att_src_W(src_enc) # bs, max_src_len, tgt_hid_size\n",
    "        \n",
    "        for t in range(tgt_len):\n",
    "            emb_t = emb[:, t, :]\n",
    "            x = T.cat([emb_t, att_vec], dim=-1)\n",
    "            h_t, c_t = self.decoder(x, hidden)\n",
    "\n",
    "            ctx_t, att_mat = self.luong_attention(h_t, src_enc, src_enc_att)\n",
    "            \n",
    "            # Luong eq. (5)\n",
    "            att_t = self.att_vec_W(T.cat([h_t, ctx_t], dim=1))\n",
    "            att_t = att_t.tanh()\n",
    "            att_t = self.dropout(att_t)\n",
    "            \n",
    "            # Luong eq. (6)\n",
    "            score_t = self.readout(att_t)\n",
    "            score_t = F.softmax(score_t, dim=-1)\n",
    "            \n",
    "            scores   += [score_t]\n",
    "            att_mats += [att_mat]\n",
    "            \n",
    "            # for next state t+1\n",
    "            att_vec = att_t\n",
    "            hidden  = (h_t, c_t)\n",
    "        \n",
    "        # bs, max_tgt_len, tgt_vocab_size\n",
    "        scores = T.stack(scores).permute((1, 0, 2))\n",
    "        \n",
    "        # each element: bs, max_src_len, max_tgt_len\n",
    "        att_mats = T.cat(att_mats, dim=1)\n",
    "        \n",
    "        return scores, att_mats\n",
    "            \n",
    "        \n",
    "    def luong_attention(self, h_t, src_enc, src_enc_att, mask=None):\n",
    "        \"\"\"\n",
    "        h_t               : bs, hid_size\n",
    "        src_enc (hs)      : bs, max_src_len, 2*src_hid_size \n",
    "        src_enc_att (W*hs): bs, max_src_len, tgt_hid_size\n",
    "        mask              : bs, max_src_len\n",
    "        \n",
    "        ctx_vec    : bs, 2*src_hid_size\n",
    "        att_weight : bs, max_src_len\n",
    "        att_mat    : bs, 1, max_src_len\n",
    "        \"\"\"\n",
    "        \n",
    "        # bs x src_max_len\n",
    "        score = T.bmm(src_enc_att, h_t.unsqueeze(2)).squeeze(2)\n",
    "        \n",
    "        if mask:\n",
    "            score.data.masked_fill_(mask, -np.inf)\n",
    "        \n",
    "        att_mat = score.unsqueeze(1)\n",
    "        att_weights = F.softmax(score, dim=-1)\n",
    "        \n",
    "        # sum per timestep\n",
    "        ctx_vec = T.sum(att_weights.unsqueeze(2) * src_enc, dim=1)\n",
    "        \n",
    "        return ctx_vec, att_mat\n",
    "    \n",
    "    \n",
    "    def beam_search(self, src, width=3):\n",
    "        \"\"\"\n",
    "        Choose most probable sequence, considering top `width` candidates.\n",
    "        \"\"\"\n",
    "\n",
    "        hyp = []\n",
    "\n",
    "        batch_size, src_len = src.shape\n",
    "        enc_out, (h0_dec, c0_dec) = self.encode(src)\n",
    "\n",
    "        scores, att_mats = [], []\n",
    "\n",
    "        hidden = (h0_dec, c0_dec)\n",
    "\n",
    "        att_vec = T.zeros(batch_size, self.tgt_cfg.lstm_hidden_size, requires_grad=False).cuda()\n",
    "\n",
    "        # Luong W*hs: same for each timestep of the decoder\n",
    "        src_enc_att = self.att_src_W(src_enc) # bs, max_src_len, tgt_hid_size\n",
    "\n",
    "        for t in range(tgt_len):\n",
    "            emb_t = self.tgt_embedding(hyp[-1])\n",
    "            x = T.cat([emb_t, att_vec], dim=-1)\n",
    "            h_t, c_t = self.decoder(x, hidden)\n",
    "\n",
    "            ctx_t, att_mat = self.luong_attention(h_t, src_enc, src_enc_att)\n",
    "\n",
    "            att_t = F.tanh(self.att_vec_W(T.cat([h_t, ctx_t], dim=1)))\n",
    "            # att_t = self.dropout(att_t)\n",
    "\n",
    "            score_t = F.softmax(self.readout(att_t), dim=-1)\n",
    "\n",
    "            scores   += [score_t]\n",
    "            att_mats += [att_mat]\n",
    "\n",
    "            # for next state t+1\n",
    "            att_vec = att_t\n",
    "            hidden  = (h_t, c_t)\n",
    "\n",
    "        # bs, max_tgt_len, tgt_vocab_size\n",
    "        scores = T.stack(scores).permute((1, 0, 2))\n",
    "\n",
    "        # each element: bs, max_src_len, max_tgt_len\n",
    "        att_mats = T.cat(att_mats, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "        return hyp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def JSD(a, b, mask=None):\n",
    "    eps = 1e-8\n",
    "    \n",
    "    assert a.shape == b.shape\n",
    "    _, n, _ = a.shape \n",
    "            \n",
    "    xa = F.softmax(a, dim=2) + eps\n",
    "    xb = F.softmax(b, dim=2) + eps\n",
    "    \n",
    "    # common, averaged dist\n",
    "    avg = 0.5 * (xa + xb)\n",
    "    \n",
    "    # kl\n",
    "    xa = T.sum(xa * T.log(xa / avg), dim=2)\n",
    "    xb = T.sum(xb * T.log(xb / avg), dim=2)\n",
    "    \n",
    "    # js\n",
    "    xa = T.sum(xa, dim=1) / n\n",
    "    xb = T.sum(xb, dim=1) / n\n",
    "    \n",
    "    return 0.5 * (xa + xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cg_model     = Model(CFG, model_type='cg')\n",
    "cg_model.opt = O.Adam(lr=0.001, params=filter(lambda p: p.requires_grad, cg_model.parameters()))\n",
    "\n",
    "cs_model     = Model(CFG, model_type='cs')\n",
    "cs_model.opt = O.Adam(lr=0.001, params=filter(lambda p: p.requires_grad, cs_model.parameters()))\n",
    "\n",
    "# TODO: very hacky\n",
    "n = int(CFG.train_split * len(dataset))\n",
    "train_dataset = deepcopy(dataset)\n",
    "train_dataset.anno = dataset.anno[:n]\n",
    "train_dataset.code = dataset.code[:n]\n",
    "train_dataset.df   = dataset.df.iloc[:n]\n",
    "train_dataset.lm_probs['anno'] = dataset.lm_probs['anno'][:n]\n",
    "train_dataset.lm_probs['code'] = dataset.lm_probs['code'][:n]\n",
    "# ---\n",
    "\n",
    "kwargs = {'num_workers': 4, 'pin_memory': True} if CFG.cuda else {}\n",
    "train_loader = DataLoader(train_dataset, batch_size=CFG.batch_size, shuffle=True, **kwargs)\n",
    "print(f'DataLoader: {len(train_loader)} batches of size {CFG.batch_size} (total: {len(train_dataset)})')\n",
    "\n",
    "__cg_l = 0\n",
    "__cs_l = 0\n",
    "__att_l = 0\n",
    "__dual_l = 0\n",
    "__rep_every = 50\n",
    "__tb_every = __rep_every // 4\n",
    "\n",
    "CFG.to_file(os.path.join(exp_dir, 'config.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = 0\n",
    "\n",
    "for epoch_idx in range(1, CFG.num_epochs+1):\n",
    "    \n",
    "    for batch_idx, (anno, code, anno_lm_p, code_lm_p) in enumerate(train_loader, start=1):        \n",
    "        anno_len, code_len = anno.shape[1], code.shape[1]\n",
    "        \n",
    "        if CFG.cuda:\n",
    "            anno, code, anno_lm_p, code_lm_p = map(lambda t: t.cuda(), [anno, code, anno_lm_p, code_lm_p])\n",
    "            \n",
    "        # binary mask indicating the presence of padding token\n",
    "        anno_mask = T.tensor(anno != dataset.anno_lang.token2index['<pad>']).byte()\n",
    "        code_mask = T.tensor(code != dataset.code_lang.token2index['<pad>']).byte()\n",
    "            \n",
    "        # forward pass\n",
    "        code_pred, code_att_mat = cg_model(src=anno, tgt=code)\n",
    "        anno_pred, anno_att_mat = cs_model(src=code, tgt=anno)\n",
    "                                    \n",
    "        # loss computation\n",
    "        l_cg_ce, l_cs_ce = 0, 0\n",
    "        \n",
    "        # CG cross-entropy loss\n",
    "        for t in range(code_len):\n",
    "            probs = code_pred[:, t, :].gather(1, code[:, t].view(-1, 1)).squeeze(1)\n",
    "            l_cg_ce += -T.log(probs) * code_mask[:, t] / code_len\n",
    "                    \n",
    "        # CS cross-entropy loss\n",
    "        for t in range(anno_len):\n",
    "            probs = anno_pred[:, t, :].gather(1, anno[:, t].view(-1, 1)).squeeze(1)\n",
    "            l_cs_ce += -T.log(probs) * anno_mask[:, t] / anno_len\n",
    "            \n",
    "        # dual loss: P(x,y) = P(x).P(y|x) = P(y).P(x|y)\n",
    "        l_dual = (code_lm_p - l_cs_ce - anno_lm_p + l_cg_ce) ** 2\n",
    "                \n",
    "        # attention loss: JSD\n",
    "        l_att = JSD(anno_att_mat, code_att_mat.transpose(2,1)) + \\\n",
    "                JSD(anno_att_mat.transpose(2,1), code_att_mat)\n",
    "                \n",
    "        # final loss\n",
    "        p, a = 0, 0\n",
    "        l_cg = T.mean(l_cg_ce + p * 0.5 * l_dual + a * 0.9 * l_att)\n",
    "        l_cs = T.mean(l_cs_ce + p * 0.5 * l_dual + a * 0.9 * l_att)\n",
    "                \n",
    "        # optimize CG\n",
    "        cg_model.opt.zero_grad()\n",
    "        l_cg.backward(retain_graph=True)\n",
    "        cg_model.opt.step()\n",
    "                \n",
    "        # optimize CS\n",
    "        cs_model.opt.zero_grad()\n",
    "        l_cs.backward()\n",
    "        cs_model.opt.step()\n",
    "        \n",
    "        # tensorboard\n",
    "        if batch_idx % __tb_every == 0:\n",
    "            for name, param in cg_model.named_parameters():\n",
    "                tb_writer.add_histogram(f'CG-{name}', param, ts)\n",
    "            for name, param in cs_model.named_parameters():\n",
    "                tb_writer.add_histogram(f'CS-{name}', param, ts)\n",
    "            tb_writer.add_scalar('train/CG_loss', l_cg.item(), ts)\n",
    "            tb_writer.add_scalar('train/CS_loss', l_cs.item(), ts)\n",
    "            tb_writer.add_scalar('train/ATT_loss', l_att.mean().item(), ts)\n",
    "            tb_writer.add_scalar('train/DUAL_loss', l_dual.mean().item(), ts)\n",
    "            ts += 1\n",
    "                \n",
    "        # reporting\n",
    "        __cg_l   += l_cg.item() / __rep_every\n",
    "        __cs_l   += l_cs.item() / __rep_every\n",
    "        __att_l  += l_att.mean().item()  / __rep_every\n",
    "        __dual_l += l_dual.mean().item() / __rep_every\n",
    "        \n",
    "        if batch_idx % __rep_every == 0:\n",
    "            status = [f'Epoch {epoch_idx:>5d}/{CFG.num_epochs:>3d}', f'Batch {batch_idx:>5d}/{len(train_loader):5d}',\n",
    "                      f'CG {__cg_l:7.5f}', f'CS {__cs_l:7.5f}', f'ATT {__att_l:7.5f}', f'DUAL {__dual_l:7.5f}']\n",
    "            print(' | '.join(status))\n",
    "            __cg_l, __cs_l, __att_l, __dual_l = 0, 0, 0, 0\n",
    "    # --- epoch end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(cg_model.state_dict(), os.path.join(exp_dir, 'cg_model.pt'))\n",
    "torch.save(cs_model.state_dict(), os.path.join(exp_dir, 'cs_model.pt'))\n",
    "\n",
    "tb_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cg_model = Model(CFG, model_type='cg')\n",
    "cs_model = Model(CFG, model_type='cs')\n",
    "\n",
    "# exp_dir = f'./experiments/{CFG.exp_name}'\n",
    "exp_dir = f'./experiments/{os.path.basename(DATASET_DIR)}-p{0}-a{1}-minfreq2'\n",
    "\n",
    "cg_model.load_state_dict(torch.load(os.path.join(exp_dir, 'cg_model.pt')))\n",
    "cs_model.load_state_dict(torch.load(os.path.join(exp_dir, 'cs_model.pt')))\n",
    "\n",
    "exp_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_code(line):\n",
    "    \"valid <=> (complete ^ valid) v (incomplete ^ valid_prefix)\"\n",
    "    try:\n",
    "        codeop.compile_command(line)\n",
    "    except SyntaxError:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def to_tok(xs, mode):\n",
    "    z = (xs)[0].cpu()\n",
    "    z = z[(z!=0)&(z!=1)&(z!=2)&(z!=3)]\n",
    "    if mode == 'code':\n",
    "        return dataset.code_lang.to_tokens(z)[0]\n",
    "    if mode == 'anno':\n",
    "        return dataset.anno_lang.to_tokens(z)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: very hacky\n",
    "n = int(CFG.train_split * len(dataset))\n",
    "test_dataset = deepcopy(dataset)\n",
    "test_dataset.anno = dataset.anno[n:]\n",
    "test_dataset.code = dataset.code[n:]\n",
    "test_dataset.df   = dataset.df.iloc[n:]\n",
    "test_dataset.lm_probs['anno'] = dataset.lm_probs['anno'][n:]\n",
    "test_dataset.lm_probs['code'] = dataset.lm_probs['code'][n:]\n",
    "# ---\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "assert len(test_loader) == len(dataset) - n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = ['ind_match', 'exact_match', 'coverage']\n",
    "metrics = {\n",
    "    'anno': {k: 0 for k in ms},\n",
    "    'code': {k: 0 for k in ms}\n",
    "}\n",
    "metrics['code']['pov'] = 0\n",
    "\n",
    "anno_toks, code_toks = [], []\n",
    "\n",
    "with T.no_grad():\n",
    "    cg_model.eval()\n",
    "    cs_model.eval()\n",
    "    \n",
    "    for batch_idx, (anno, code, _, _) in tqdm(enumerate(test_loader, start=1), total=len(test_loader)): \n",
    "        if CFG.cuda:\n",
    "            anno, code = anno.cuda(), code.cuda() \n",
    "            \n",
    "        # binary mask indicating the presence of padding token\n",
    "#         anno_mask = T.tensor(anno != dataset.anno_lang.token2index['<pad>']).byte()\n",
    "#         code_mask = T.tensor(code != dataset.code_lang.token2index['<pad>']).byte()\n",
    "\n",
    "        anno_mask = T.tensor((anno != 0) * (anno != 1)).byte()\n",
    "        code_mask = T.tensor((code != 0) * (code != 1)).byte()\n",
    "            \n",
    "        # forward pass\n",
    "        code_pred, code_att_mat = cg_model(src=anno, tgt=code)\n",
    "        anno_pred, anno_att_mat = cs_model(src=code, tgt=anno)\n",
    "        \n",
    "        # TODO: ideally, this should be beam-search\n",
    "        code_pred = code_pred.argmax(dim=2)\n",
    "        anno_pred = anno_pred.argmax(dim=2)\n",
    "        \n",
    "        code_score = (((code_pred == code) * code_mask).float().sum() / code_mask.sum()).cpu()\n",
    "        anno_score = (((anno_pred == anno) * anno_mask).float().sum() / anno_mask.sum()).cpu()\n",
    "        \n",
    "        # 1)\n",
    "        metrics['code']['ind_match'] += code_score / len(test_loader)\n",
    "        metrics['anno']['ind_match'] += anno_score / len(test_loader)\n",
    "        \n",
    "        # 2)\n",
    "        if np.isclose(code_score, 1):\n",
    "            metrics['code']['exact_match'] += 1 / len(test_loader)\n",
    "        if np.isclose(anno_score, 1):\n",
    "            metrics['anno']['exact_match'] += 1 / len(test_loader)\n",
    "            \n",
    "        # 3)\n",
    "        sy  = set([x.item() for x in (code * code_mask)[0].cpu().data if x.item() != 0])\n",
    "        sy_ = set([x.item() for x in (code_pred * code_mask)[0].cpu().data if x.item() != 0])\n",
    "        if len(set.difference(sy_, sy)) == 0:\n",
    "            metrics['code']['coverage'] += 1 / len(test_loader)\n",
    "        else:\n",
    "            if np.isclose(code_score, 1):\n",
    "                print(set.difference(sy_, sy))\n",
    "            \n",
    "        sy  = set([x.item() for x in (anno * anno_mask)[0].cpu().data if x.item() != 0])\n",
    "        sy_ = set([x.item() for x in (anno_pred * anno_mask)[0].cpu().data if x.item() != 0])\n",
    "        if len(set.difference(sy_, sy)) == 0:\n",
    "            metrics['anno']['coverage'] += 1 / len(test_loader)\n",
    "            \n",
    "        # 4)\n",
    "        if is_valid_code(' '.join(to_tok(code_pred * code_mask, 'code'))):\n",
    "            metrics['code']['pov'] += 1 / len(test_loader)\n",
    "\n",
    "        # save tokens\n",
    "        code_toks += [(round(code_score.item(), 5), \n",
    "                       to_tok(code_pred * code_mask, 'code'), \n",
    "                       to_tok(code * code_mask, 'code'),\n",
    "                       code_pred[0].cpu(),\n",
    "                       code[0].cpu())]\n",
    "        \n",
    "        anno_toks += [(round(anno_score.item(), 5), \n",
    "                       to_tok(anno_pred * anno_mask, 'anno'), \n",
    "                       to_tok(anno * anno_mask, 'anno'),\n",
    "                       anno_pred[0].cpu(),\n",
    "                       anno[0].cpu())]\n",
    "            \n",
    "code_toks = sorted(code_toks, key=lambda x: x[0])\n",
    "anno_toks = sorted(anno_toks, key=lambda x: x[0])\n",
    "\n",
    "with open(os.path.join(exp_dir, 'eval_code.txt'), 'wt') as fp:\n",
    "    for i, (s, pt, tt, p, t) in enumerate(code_toks, start=1):\n",
    "        fp.write(f'{i}\\n')\n",
    "        fp.write(f'{s}\\n')\n",
    "        fp.write(f'pred: {\" \".join(pt)}\\n')\n",
    "        fp.write(f'true: {\" \".join(tt)}\\n')\n",
    "        fp.write(f'pred_raw: {p}\\n')\n",
    "        fp.write(f'true_raw: {t}\\n')\n",
    "        fp.write(f'{\"-\"*80}\\n')\n",
    "        \n",
    "with open(os.path.join(exp_dir, 'eval_anno.txt'), 'wt') as fp:\n",
    "    for i, (s, pt, tt, p, t) in enumerate(anno_toks, start=1):\n",
    "        fp.write(f'{i}\\n')\n",
    "        fp.write(f'{s}\\n')\n",
    "        fp.write(f'pred: {\" \".join(pt)}\\n')\n",
    "        fp.write(f'true: {\" \".join(tt)}\\n')\n",
    "        fp.write(f'pred_raw: {p}\\n')\n",
    "        fp.write(f'true_raw: {t}\\n')\n",
    "        fp.write(f'{\"-\"*80}\\n')\n",
    "\n",
    "# results\n",
    "print(exp_dir.split('/')[-1])\n",
    "print(len(test_loader))\n",
    "for k in ms:\n",
    "    print(f\"{metrics['anno'][k]:7.5f}/{metrics['code'][k]:7.5f}\", end=' ')\n",
    "print(round(metrics['code']['pov'], 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Attention matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = T.tensor([  2, 576,  16,  84, 474, 695,   0,   0,   0,   3])\n",
    "c = T.tensor([  2, 155, 489,  10, 159,   5,   8,   0,   0,   3])\n",
    "\n",
    "with T.no_grad():\n",
    "    i = np.random.randint(len(test_dataset))\n",
    "#     i = 5557\n",
    "    a, c, _, _ = test_dataset[-1]\n",
    "    a, c = a.cuda(), c.cuda()\n",
    "    anno_mask = T.tensor((a != 0) * (a != 1)).byte().cuda()\n",
    "    code_mask = T.tensor((c != 0) * (c != 1)).byte().cuda()\n",
    "    x, x_mat = cg_model(src=a.unsqueeze(0), tgt=c.unsqueeze(0))\n",
    "    y, y_mat = cs_model(src=c.unsqueeze(0), tgt=a.unsqueeze(0))\n",
    "    x = x[0].argmax(dim=-1)\n",
    "    x_mat = x_mat[0].cpu()\n",
    "    y = y[0].argmax(dim=-1)\n",
    "    y_mat = y_mat[0].cpu()\n",
    "    \n",
    "    ct = to_tok((c * code_mask).unsqueeze(0), 'code')\n",
    "    xt = to_tok((x * code_mask).unsqueeze(0), 'code')\n",
    "    at = to_tok((a * anno_mask).unsqueeze(0), 'anno')\n",
    "    yt = to_tok((y * anno_mask).unsqueeze(0), 'anno')\n",
    "    \n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# plt.subplot(1, 2, 1)\n",
    "plt.imshow(F.softmax(y_mat, -1), cmap='jet')\n",
    "plt.grid(False)\n",
    "plt.xticks(np.arange(len(ct)), labels=ct, rotation=90)\n",
    "plt.yticks(np.arange(len(at)), labels=at)\n",
    "\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.imshow(F.softmax(y_mat, -1), cmap='jet')\n",
    "# plt.grid(False)\n",
    "# plt.yticks(np.arange(len(ct)), labels=ct)\n",
    "# plt.xticks(np.arange(len(at)), labels=at, rotation=90)\n",
    "\n",
    "pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
